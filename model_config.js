// EMD Model Configuration
// Auto-generated from Python model definitions
// Generated at: 2025-10-16T11:10:40.195799

window.EMD_MODEL_CONFIG = {
  "metadata": {
    "generated_at": "2025-10-16T11:10:40.195799",
    "version": "1.0.0",
    "source": "EMD Python Model Definitions"
  },
  "models": {
    "glm-4-9b-chat": {
      "model_id": "glm-4-9b-chat",
      "model_type": "llm",
      "description": "glm-4-9b-chat-GPTQ-Int4",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/glm-4-9b-chat",
      "modelscope_model_id": "ZhipuAI/glm-4-9b-chat",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "GLM-4-9B-0414": {
      "model_id": "GLM-4-9B-0414",
      "model_type": "llm",
      "description": "GLM-4-32B-0414 series",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/GLM-4-9B-0414",
      "modelscope_model_id": "ZhipuAI/GLM-4-9B-0414",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "GLM-4-32B-0414": {
      "model_id": "GLM-4-32B-0414",
      "model_type": "llm",
      "description": "GLM-4-32B-0414 series",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/GLM-4-32B-0414",
      "modelscope_model_id": "ZhipuAI/GLM-4-32B-0414",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "GLM-Z1-9B-0414": {
      "model_id": "GLM-Z1-9B-0414",
      "model_type": "llm",
      "description": "GLM-4-32B-0414 series",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/GLM-Z1-9B-0414",
      "modelscope_model_id": "ZhipuAI/GLM-Z1-9B-0414",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "GLM-Z1-32B-0414": {
      "model_id": "GLM-Z1-32B-0414",
      "model_type": "llm",
      "description": "GLM-4-32B-0414 series",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/GLM-Z1-32B-0414",
      "modelscope_model_id": "ZhipuAI/GLM-Z1-32B-0414",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "GLM-Z1-Rumination-32B-0414": {
      "model_id": "GLM-Z1-Rumination-32B-0414",
      "model_type": "llm",
      "description": "GLM-4-32B-0414 series",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "THUDM/GLM-Z1-Rumination-32B-0414",
      "modelscope_model_id": "ZhipuAI/GLM-Z1-Rumination-32B-0414",
      "model_series": {
        "name": "glm4",
        "description": "The GLM-4 series includes the latest generation of pre-trained models launched by Zhipu AI.",
        "reference_link": "https://github.com/THUDM/GLM-4"
      }
    },
    "internlm2_5-20b-chat-4bit-awq": {
      "model_id": "internlm2_5-20b-chat-4bit-awq",
      "model_type": "llm",
      "description": "internlm2_5-20b-chat-4bit-awq",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "internlm/internlm2_5-20b-chat-4bit-awq",
      "modelscope_model_id": "ticoAg/internlm2_5-20b-chat-awq",
      "model_series": {
        "name": "internlm2.5",
        "description": "InternLM2.5 has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n- Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.\n\n- Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",
        "reference_link": "https://github.com/InternLM/InternLM"
      }
    },
    "internlm2_5-20b-chat": {
      "model_id": "internlm2_5-20b-chat",
      "model_type": "llm",
      "description": "internlm2_5-20b-chat",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "internlm/internlm2_5-20b-chat",
      "modelscope_model_id": "Shanghai_AI_Laboratory/internlm2_5-20b-chat",
      "model_series": {
        "name": "internlm2.5",
        "description": "InternLM2.5 has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n- Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.\n\n- Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",
        "reference_link": "https://github.com/InternLM/InternLM"
      }
    },
    "internlm2_5-7b-chat": {
      "model_id": "internlm2_5-7b-chat",
      "model_type": "llm",
      "description": "internlm2_5-7b-chat",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "internlm/internlm2_5-7b-chat",
      "modelscope_model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat",
      "model_series": {
        "name": "internlm2.5",
        "description": "InternLM2.5 has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n- Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.\n\n- Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",
        "reference_link": "https://github.com/InternLM/InternLM"
      }
    },
    "internlm2_5-7b-chat-4bit": {
      "model_id": "internlm2_5-7b-chat-4bit",
      "model_type": "llm",
      "description": "internlm2_5-7b-chat-4bit",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "internlm/internlm2_5-7b-chat-4bit",
      "modelscope_model_id": "",
      "model_series": {
        "name": "internlm2.5",
        "description": "InternLM2.5 has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n- Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.\n\n- Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",
        "reference_link": "https://github.com/InternLM/InternLM"
      }
    },
    "internlm2_5-1_8b-chat": {
      "model_id": "internlm2_5-1_8b-chat",
      "model_type": "llm",
      "description": "internlm2_5-1_8b-chat",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "internlm/internlm2_5-1_8b-chat",
      "modelscope_model_id": "Shanghai_AI_Laboratory/internlm2_5-1_8b-chat",
      "model_series": {
        "name": "internlm2.5",
        "description": "InternLM2.5 has open-sourced a 20 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\n- Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-27B.\n\n- Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation has be released in MindSearch. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.",
        "reference_link": "https://github.com/InternLM/InternLM"
      }
    },
    "gpt-oss-20b": {
      "model_id": "gpt-oss-20b",
      "model_type": "llm",
      "description": "GPT-OSS (GPT Open Source Software) is OpenAI's initiative to provide open-source AI models, making advanced language models accessible to developers, researchers, and organizations. These models are designed for building, experimenting, and scaling generative AI applications while fostering innovation and collaboration in the open-source AI community.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "openai/gpt-oss-20b",
      "modelscope_model_id": "openai/gpt-oss-20b",
      "model_series": {
        "name": "gptoss",
        "description": "GPT-OSS (GPT Open Source Software) is OpenAI's initiative to provide open-source AI models, making advanced language models accessible to developers, researchers, and organizations for building, experimenting, and scaling generative AI applications. These models are designed to foster innovation and collaboration in the open-source AI community.",
        "reference_link": "https://openai.com/index/introducing-gpt-oss/"
      }
    },
    "gpt-oss-120b": {
      "model_id": "gpt-oss-120b",
      "model_type": "llm",
      "description": "GPT-OSS (GPT Open Source Software) is OpenAI's initiative to provide open-source AI models, making advanced language models accessible to developers, researchers, and organizations. These models are designed for building, experimenting, and scaling generative AI applications while fostering innovation and collaboration in the open-source AI community.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "openai/gpt-oss-120b",
      "modelscope_model_id": "openai/gpt-oss-120b",
      "model_series": {
        "name": "gptoss",
        "description": "GPT-OSS (GPT Open Source Software) is OpenAI's initiative to provide open-source AI models, making advanced language models accessible to developers, researchers, and organizations for building, experimenting, and scaling generative AI applications. These models are designed to foster innovation and collaboration in the open-source AI community.",
        "reference_link": "https://openai.com/index/introducing-gpt-oss/"
      }
    },
    "Qwen2.5-7B-Instruct": {
      "model_id": "Qwen2.5-7B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "inf2.8xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-7B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-7B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-72B-Instruct-AWQ": {
      "model_id": "Qwen2.5-72B-Instruct-AWQ",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "inf2.24xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-72B-Instruct-AWQ",
      "modelscope_model_id": "Qwen/Qwen2.5-72B-Instruct-AWQ",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-72B-Instruct": {
      "model_id": "Qwen2.5-72B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-72B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-72B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-72B-Instruct-AWQ-128k": {
      "model_id": "Qwen2.5-72B-Instruct-AWQ-128k",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-72B-Instruct-AWQ",
      "modelscope_model_id": "Qwen/Qwen2.5-72B-Instruct-AWQ",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-32B-Instruct": {
      "model_id": "Qwen2.5-32B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-32B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-32B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-0.5B-Instruct": {
      "model_id": "Qwen2.5-0.5B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "inf2.8xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-0.5B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-0.5B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-1.5B-Instruct": {
      "model_id": "Qwen2.5-1.5B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-1.5B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-1.5B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-3B-Instruct": {
      "model_id": "Qwen2.5-3B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-3B-Instruct",
      "modelscope_model_id": "",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-14B-Instruct-AWQ": {
      "model_id": "Qwen2.5-14B-Instruct-AWQ",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "g4dn.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-14B-Instruct-AWQ",
      "modelscope_model_id": "Qwen/Qwen2.5-14B-Instruct-AWQ",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "Qwen2.5-14B-Instruct": {
      "model_id": "Qwen2.5-14B-Instruct",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-14B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-14B-Instruct",
      "model_series": {
        "name": "qwen2.5",
        "description": "Qwen2.5 language models, including pretrained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B.",
        "reference_link": "https://github.com/QwenLM/Qwen2.5"
      }
    },
    "QwQ-32B-Preview": {
      "model_id": "QwQ-32B-Preview",
      "model_type": "llm",
      "description": "large reasoning model provide by qwen team",
      "application_scenario": "large reasoning model",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/QwQ-32B-Preview",
      "modelscope_model_id": "Qwen/QwQ-32B-Preview",
      "model_series": {
        "name": "qwen reasoning model",
        "description": "Qwen Reasoning Model is a series of models that can be used to solve reasoning problems launched by Qwen team.",
        "reference_link": "https://qwenlm.github.io/zh/blog/qwq-32b-preview/\nhttps://qwenlm.github.io/zh/blog/qvq-72b-preview/"
      }
    },
    "QwQ-32B": {
      "model_id": "QwQ-32B",
      "model_type": "llm",
      "description": "large reasoning model provide by qwen team",
      "application_scenario": "large reasoning model",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/QwQ-32B",
      "modelscope_model_id": "Qwen/QwQ-32B",
      "model_series": {
        "name": "qwen reasoning model",
        "description": "Qwen Reasoning Model is a series of models that can be used to solve reasoning problems launched by Qwen team.",
        "reference_link": "https://qwenlm.github.io/zh/blog/qwq-32b-preview/\nhttps://qwenlm.github.io/zh/blog/qvq-72b-preview/"
      }
    },
    "Qwen3-8B": {
      "model_id": "Qwen3-8B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "g4dn.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-8B",
      "modelscope_model_id": "Qwen/Qwen3-8B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-0.6B": {
      "model_id": "Qwen3-0.6B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "g4dn.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-0.6B",
      "modelscope_model_id": "Qwen/Qwen3-0.6B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-1.7B": {
      "model_id": "Qwen3-1.7B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-1.7B",
      "modelscope_model_id": "Qwen/Qwen3-1.7B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-4B": {
      "model_id": "Qwen3-4B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "g4dn.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-4B",
      "modelscope_model_id": "Qwen/Qwen3-4B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-14B-AWQ": {
      "model_id": "Qwen3-14B-AWQ",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "g4dn.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-14B-AWQ",
      "modelscope_model_id": "Qwen/Qwen3-14B-AWQ",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-14B": {
      "model_id": "Qwen3-14B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-14B",
      "modelscope_model_id": "Qwen/Qwen3-14B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-32B-AWQ": {
      "model_id": "Qwen3-32B-AWQ",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-32B-AWQ",
      "modelscope_model_id": "Qwen/Qwen3-32B-AWQ",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-32B": {
      "model_id": "Qwen3-32B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-32B",
      "modelscope_model_id": "Qwen/Qwen3-32B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-30B-A3B": {
      "model_id": "Qwen3-30B-A3B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-30B-A3B",
      "modelscope_model_id": "Qwen/Qwen3-30B-A3B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-30B-A3B-Instruct-2507": {
      "model_id": "Qwen3-30B-A3B-Instruct-2507",
      "model_type": "llm",
      "description": "Qwen3-30B-A3B-Instruct-2507 is an updated instruction-tuned version featuring significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage, with enhanced 256K long-context understanding and better alignment with user preferences.",
      "application_scenario": "Agent, tool use, translation, summary, instruction following, reasoning",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "modelscope_model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-30B-A3B-Thinking-2507": {
      "model_id": "Qwen3-30B-A3B-Thinking-2507",
      "model_type": "llm",
      "description": "Qwen3-30B-A3B-Thinking-2507 is an enhanced thinking-enabled version featuring significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks. This model supports only thinking mode with enhanced 256K long-context understanding and markedly better general capabilities for highly complex reasoning tasks.",
      "application_scenario": "Advanced reasoning, mathematics, science, coding, logical reasoning, complex problem solving",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "modelscope_model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-235B-A22B": {
      "model_id": "Qwen3-235B-A22B",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-235B-A22B",
      "modelscope_model_id": "Qwen/Qwen3-235B-A22B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-235B-A22B-FP8": {
      "model_id": "Qwen3-235B-A22B-FP8",
      "model_type": "llm",
      "description": "The latest series of Qwen LLMs, offers base and tuned models from 0.5B to 72B\n parameters, featuring enhanced knowledge, improved coding and math skills, better instruction\n following, long-text generation, structured data handling, 128K token context support, and\n multilingual capabilities for 29+ languages.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-235B-A22B-FP8",
      "modelscope_model_id": "Qwen/Qwen3-235B-A22B-FP8",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "llama-3.3-70b-instruct-awq": {
      "model_id": "llama-3.3-70b-instruct-awq",
      "model_type": "llm",
      "description": "The latest series of Llama LLMs",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "casperhansen/llama-3.3-70b-instruct-awq",
      "modelscope_model_id": "",
      "model_series": {
        "name": "llama",
        "description": "Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n\n- Open access: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n- Broad ecosystem: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n- Trust & safety: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.",
        "reference_link": "https://github.com/meta-llama/llama-models"
      }
    },
    "DeepSeek-R1-Distill-Qwen-32B": {
      "model_id": "DeepSeek-R1-Distill-Qwen-32B",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-14B": {
      "model_id": "DeepSeek-R1-Distill-Qwen-14B",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-7B": {
      "model_id": "DeepSeek-R1-Distill-Qwen-7B",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-1.5B": {
      "model_id": "DeepSeek-R1-Distill-Qwen-1.5B",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-1.5B_ollama": {
      "model_id": "DeepSeek-R1-Distill-Qwen-1.5B_ollama",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "ollama"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "",
      "modelscope_model_id": "",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-1.5B-GGUF": {
      "model_id": "DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "llama.cpp"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF",
      "modelscope_model_id": "",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Qwen-32B-GGUF": {
      "model_id": "DeepSeek-R1-Distill-Qwen-32B-GGUF",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "local"
      ],
      "supported_engines": [
        "llama.cpp"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF",
      "modelscope_model_id": "",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-Distill-Llama-8B": {
      "model_id": "DeepSeek-R1-Distill-Llama-8B",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1-0528-Qwen3-8B": {
      "model_id": "DeepSeek-R1-0528-Qwen3-8B",
      "model_type": "llm",
      "description": "DeepSeek R1 got a minor upgrade (now DeepSeek-R1-0528). It does great in math, programming, and logic tests, almost as good as top models like O3 and Gemini 2.5 Pro.",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "modelscope_model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "deepseek-r1-distill-llama-70b-awq": {
      "model_id": "deepseek-r1-distill-llama-70b-awq",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "casperhansen/deepseek-r1-distill-llama-70b-awq",
      "modelscope_model_id": "",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "deepseek-r1-671b-1.58bit_gguf": {
      "model_id": "deepseek-r1-671b-1.58bit_gguf",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g6.8xlarge",
        "g6.12xlarge",
        "g6.16xlarge",
        "g6.24xlarge",
        "g6.48xlarge",
        "g6e.4xlarge",
        "g6e.8xlarge",
        "g6e.12xlarge",
        "g6e.16xlarge",
        "g6e.24xlarge",
        "g6e.48xlarge",
        "local"
      ],
      "supported_engines": [
        "llama.cpp",
        "ktransformers"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1-GGUF",
      "modelscope_model_id": "unsloth/DeepSeek-R1-GGUF",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "deepseek-r1-671b-2.51bit_gguf": {
      "model_id": "deepseek-r1-671b-2.51bit_gguf",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g6.12xlarge",
        "g6.16xlarge",
        "g6.24xlarge",
        "g6.48xlarge",
        "g6e.8xlarge",
        "g6e.12xlarge",
        "g6e.16xlarge",
        "g6e.24xlarge",
        "g6e.48xlarge",
        "local"
      ],
      "supported_engines": [
        "ktransformers"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1-GGUF",
      "modelscope_model_id": "unsloth/DeepSeek-R1-GGUF",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "DeepSeek-R1": {
      "model_id": "DeepSeek-R1",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1",
      "modelscope_model_id": "unsloth/DeepSeek-R1",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "deepseek-r1-671b-4bit_gguf": {
      "model_id": "deepseek-r1-671b-4bit_gguf",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs for reasoning",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.24xlarge",
        "g5.48xlarge",
        "g6.24xlarge",
        "g6.48xlarge",
        "g6e.16xlarge",
        "g6e.24xlarge",
        "g6e.48xlarge",
        "local"
      ],
      "supported_engines": [
        "ktransformers"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "unsloth/DeepSeek-R1-GGUF",
      "modelscope_model_id": "unsloth/DeepSeek-R1-GGUF",
      "model_series": {
        "name": "deepseek reasoning model",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "deepseek-v3-UD-IQ1_M_ollama": {
      "model_id": "deepseek-v3-UD-IQ1_M_ollama",
      "model_type": "llm",
      "description": "The latest series of DeepSeek LLMs",
      "application_scenario": "Agent, tool use, translation, summary",
      "supported_instances": [
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "ollama"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "",
      "modelscope_model_id": "",
      "model_series": {
        "name": "deepseek v3",
        "description": "DeepSeek-R1-Zero and DeepSeek-R1 are innovative reasoning models, with the former showcasing strong performance through reinforcement learning alone, while the latter enhances reasoning capabilities by incorporating cold-start data, achieving results comparable to OpenAI-o1 and setting new benchmarks with its distilled versions.",
        "reference_link": "https://github.com/deepseek-ai/DeepSeek-R1"
      }
    },
    "Baichuan-M1-14B-Instruct": {
      "model_id": "Baichuan-M1-14B-Instruct",
      "model_type": "llm",
      "description": "medical domain LLM",
      "application_scenario": "chat/translation",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "huggingface"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "baichuan-inc/Baichuan-M1-14B-Instruct",
      "modelscope_model_id": "",
      "model_series": {
        "name": "baichuan",
        "description": "Baichuan Intelligent Technology.",
        "reference_link": "https://github.com/baichuan-inc"
      }
    },
    "ReaderLM-v2": {
      "model_id": "ReaderLM-v2",
      "model_type": "llm",
      "description": "ReaderLM-v2 is a 1.5B parameter language model that converts raw HTML into beautifully formatted markdown or JSON with superior accuracy and improved longer context handling. Supporting multiple languages (29 in total), ReaderLM-v2 is specialized for tasks involving HTML parsing, transformation, and text extraction.",
      "application_scenario": "Html information extraction",
      "supported_instances": [
        "g4dn.2xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "inf2.8xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm",
        "tgi"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "jinaai/ReaderLM-v2",
      "modelscope_model_id": "jinaai/ReaderLM-v2",
      "model_series": {
        "name": "jina",
        "description": "Search foundation models: embeddings, rerankers, small LMs for better search",
        "reference_link": "https://huggingface.co/jinaai"
      }
    },
    "txgemma-9b-chat": {
      "model_id": "txgemma-9b-chat",
      "model_type": "llm",
      "description": "The latest series of txgemma",
      "application_scenario": "llms for the development of therapeutics.",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "google/txgemma-9b-chat",
      "modelscope_model_id": "AI-ModelScope/txgemma-9b-chat",
      "model_series": {
        "name": "txgemma",
        "description": "TXGemma is a series of open models to accelerate the development of therapeutics.",
        "reference_link": "https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"
      }
    },
    "txgemma-27b-chat": {
      "model_id": "txgemma-27b-chat",
      "model_type": "llm",
      "description": "The latest series of txgemma",
      "application_scenario": "llms for the development of therapeutics.",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "google/txgemma-27b-chat",
      "modelscope_model_id": "AI-ModelScope/txgemma-27b-chat",
      "model_series": {
        "name": "txgemma",
        "description": "TXGemma is a series of open models to accelerate the development of therapeutics.",
        "reference_link": "https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"
      }
    },
    "medgemma-27b-text-it": {
      "model_id": "medgemma-27b-text-it",
      "model_type": "llm",
      "description": "The latest series of medgemma",
      "application_scenario": "llm for medical text and image comprehension",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "google/medgemma-27b-text-it",
      "modelscope_model_id": "google/medgemma-27b-text-it",
      "model_series": {
        "name": "medgemma",
        "description": "MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension.",
        "reference_link": "https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"
      }
    },
    "medgemma-4b-it": {
      "model_id": "medgemma-4b-it",
      "model_type": "llm",
      "description": "The latest series of medgemma",
      "application_scenario": "llm for medical text and image comprehension",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "google/medgemma-4b-it",
      "modelscope_model_id": "google/medgemma-4b-it",
      "model_series": {
        "name": "medgemma",
        "description": "MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension.",
        "reference_link": "https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"
      }
    },
    "Qwen2-VL-72B-Instruct-AWQ": {
      "model_id": "Qwen2-VL-72B-Instruct-AWQ",
      "model_type": "vlm",
      "description": "The latest series of Qwen2 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2-VL-72B-Instruct-AWQ",
      "modelscope_model_id": "Qwen/Qwen2-VL-72B-Instruct-AWQ",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "Qwen2.5-VL-72B-Instruct-AWQ": {
      "model_id": "Qwen2.5-VL-72B-Instruct-AWQ",
      "model_type": "vlm",
      "description": "The latest series of Qwen2.5 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ",
      "modelscope_model_id": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "Qwen2.5-VL-72B-Instruct": {
      "model_id": "Qwen2.5-VL-72B-Instruct",
      "model_type": "vlm",
      "description": "The latest series of Qwen2.5 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.48xlarge",
        "g6e.12xlarge",
        "g6e.24xlarge",
        "g6e.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-VL-72B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-VL-72B-Instruct",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "Qwen2.5-VL-32B-Instruct": {
      "model_id": "Qwen2.5-VL-32B-Instruct",
      "model_type": "vlm",
      "description": "The latest series of Qwen2.5 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-VL-32B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-VL-32B-Instruct",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "Qwen2.5-VL-7B-Instruct": {
      "model_id": "Qwen2.5-VL-7B-Instruct",
      "model_type": "vlm",
      "description": "The latest series of Qwen2.5 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g6e.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2.5-VL-7B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2.5-VL-7B-Instruct",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "QVQ-72B-Preview-AWQ": {
      "model_id": "QVQ-72B-Preview-AWQ",
      "model_type": "vlm",
      "description": "The latest reasoning model of Qwen2 VL",
      "application_scenario": "vision llms for complex image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "kosbu/QVQ-72B-Preview-AWQ",
      "modelscope_model_id": "",
      "model_series": {
        "name": "qwen reasoning model",
        "description": "Qwen Reasoning Model is a series of models that can be used to solve reasoning problems launched by Qwen team.",
        "reference_link": "https://qwenlm.github.io/zh/blog/qwq-32b-preview/\nhttps://qwenlm.github.io/zh/blog/qvq-72b-preview/"
      }
    },
    "Qwen2-VL-7B-Instruct": {
      "model_id": "Qwen2-VL-7B-Instruct",
      "model_type": "vlm",
      "description": "The latest series of Qwen2 VL",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g6e.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen2-VL-7B-Instruct",
      "modelscope_model_id": "Qwen/Qwen2-VL-7B-Instruct",
      "model_series": {
        "name": "qwen2vl",
        "description": "Qwen2-VL is the latest version of the vision language models in the Qwen model families.\n\nKey Enhancements:\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: with the online streaming capabilities, Qwen2-VL can understand videos over 20 minutes by high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.",
        "reference_link": "https://github.com/QwenLM/Qwen2-VL"
      }
    },
    "Qwen3-VL-30B-A3B-Instruct": {
      "model_id": "Qwen3-VL-30B-A3B-Instruct",
      "model_type": "vlm",
      "description": "Qwen3 VL 30B model with advanced vision-language capabilities, reasoning support, and enhanced multimodal understanding",
      "application_scenario": "vision llms for advanced image understanding and reasoning",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct",
      "modelscope_model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "UI-TARS-1.5-7B": {
      "model_id": "UI-TARS-1.5-7B",
      "model_type": "vlm",
      "description": "The latest series of UI-TARS-1.5 from ByteDance-Seed team",
      "application_scenario": "computer-use or browser-use",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "g6e.2xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "ByteDance-Seed/UI-TARS-1.5-7B",
      "modelscope_model_id": "ByteDance-Seed/UI-TARS-1.5-7B",
      "model_series": {
        "name": "agent",
        "description": "LLM or VLM models for Agentic tasks, e.g. computer-use,brower-use",
        "reference_link": ""
      }
    },
    "InternVL2_5-78B-AWQ": {
      "model_id": "InternVL2_5-78B-AWQ",
      "model_type": "vlm",
      "description": "The latest series of Intervl2.5",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "lmdeploy"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "OpenGVLab/InternVL2_5-78B-AWQ",
      "modelscope_model_id": "",
      "model_series": {
        "name": "internvl2.5",
        "description": "InternVL2.5 is an advanced multimodal large language model (MLLM) series with parameter coverage ranging from 1B to 78B. InternVL2_5-78B is the first open-source MLLMs to achieve over 70% on the MMMU benchmark, matching the performance of leading closed-source commercial models like GPT-4o.",
        "reference_link": "https://github.com/OpenGVLab/InternVL"
      }
    },
    "gemma-3-4b-it": {
      "model_id": "gemma-3-4b-it",
      "model_type": "vlm",
      "description": "The latest series of Gemma 3",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "",
      "modelscope_model_id": "LLM-Research/gemma-3-4b-it",
      "model_series": {
        "name": "gemma3",
        "description": "Gemma 3 is Google’s latest open-source multimodal AI model, supporting text, image, and video processing with up to 128K tokens of context. It comes in 1B, 4B, 12B, and 27B parameter sizes, offering high efficiency, with the largest model running on a single H100 GPU. Ranking among top AI models, Gemma 3 excels in multilingual tasks, function calling, and long-document understanding, making it ideal for diverse AI applications.",
        "reference_link": "https://blog.google/technology/developers/gemma-3/"
      }
    },
    "gemma-3-12b-it": {
      "model_id": "gemma-3-12b-it",
      "model_type": "vlm",
      "description": "The latest series of Gemma 3",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "",
      "modelscope_model_id": "LLM-Research/gemma-3-12b-it",
      "model_series": {
        "name": "gemma3",
        "description": "Gemma 3 is Google’s latest open-source multimodal AI model, supporting text, image, and video processing with up to 128K tokens of context. It comes in 1B, 4B, 12B, and 27B parameter sizes, offering high efficiency, with the largest model running on a single H100 GPU. Ranking among top AI models, Gemma 3 excels in multilingual tasks, function calling, and long-document understanding, making it ideal for diverse AI applications.",
        "reference_link": "https://blog.google/technology/developers/gemma-3/"
      }
    },
    "gemma-3-27b-it": {
      "model_id": "gemma-3-27b-it",
      "model_type": "vlm",
      "description": "The latest series of Gemma 3",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "",
      "modelscope_model_id": "LLM-Research/gemma-3-27b-it",
      "model_series": {
        "name": "gemma3",
        "description": "Gemma 3 is Google’s latest open-source multimodal AI model, supporting text, image, and video processing with up to 128K tokens of context. It comes in 1B, 4B, 12B, and 27B parameter sizes, offering high efficiency, with the largest model running on a single H100 GPU. Ranking among top AI models, Gemma 3 excels in multilingual tasks, function calling, and long-document understanding, making it ideal for diverse AI applications.",
        "reference_link": "https://blog.google/technology/developers/gemma-3/"
      }
    },
    "Mistral-Small-3.1-24B-Instruct-2503": {
      "model_id": "Mistral-Small-3.1-24B-Instruct-2503",
      "model_type": "vlm",
      "description": "The latest series of mistral small",
      "application_scenario": "vision llms for image understanding",
      "supported_instances": [
        "g5.12xlarge",
        "g5.24xlarge",
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "unsloth/Mistral-Small-3.1-24B-Instruct-2503",
      "modelscope_model_id": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "model_series": {
        "name": "mistral",
        "description": "LLMs and VLMs provided by MISTRAL AI.",
        "reference_link": "https://huggingface.co/mistralai"
      }
    },
    "dotsocr": {
      "model_id": "dotsocr",
      "model_type": "vlm",
      "description": "dots.ocr is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model. Built on a compact 1.7B-parameter LLM foundation, it achieves state-of-the-art performance on text, tables, and reading order tasks with support for over 100 languages including English, Chinese, and many others.",
      "application_scenario": "multilingual document layout parsing, OCR, document understanding, table extraction, formula recognition, reading order detection",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "sagemaker_async",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "rednote-hilab/dots.ocr",
      "modelscope_model_id": "rednote-hilab/dots.ocr",
      "model_series": {
        "name": "dots_ocr",
        "description": "dots.ocr is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance on text, tables, and reading order tasks with multilingual support for over 100 languages.",
        "reference_link": "https://github.com/rednote-hilab/dots.ocr"
      }
    },
    "txt2video-LTX": {
      "model_id": "txt2video-LTX",
      "model_type": "video",
      "description": "video generation with LTX model,LTX-Video can generate high-quality videos in real-time. It can generate 24 FPS videos at 768x512 resolution",
      "application_scenario": "txt to image and image to video",
      "supported_instances": [
        "g5.4xlarge",
        "g5.8xlarge",
        "g6e.2xlarge"
      ],
      "supported_engines": [
        "comfyui"
      ],
      "supported_services": [
        "sagemaker_async"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "",
      "modelscope_model_id": "",
      "model_series": {
        "name": "comfyui",
        "description": "ComfyUI is a series of models that can be used to generate images from text prompts.",
        "reference_link": ""
      }
    },
    "whisper": {
      "model_id": "whisper",
      "model_type": "whisper",
      "description": "The Whisper model is an advanced speech-to-text system developed by OpenAI, capable of transcribing audio in multiple languages with high accuracy. It is designed to handle various audio inputs, including noisy environments, and can also translate spoken language into text.",
      "application_scenario": "asr",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge"
      ],
      "supported_engines": [
        "huggingface"
      ],
      "supported_services": [
        "sagemaker_async"
      ],
      "allow_china_region": false,
      "huggingface_model_id": "",
      "modelscope_model_id": "",
      "model_series": {
        "name": "whisper",
        "description": "Whisper includes both English-only and multilingual checkpoints for ASR and ST, ranging from 38M params for the tiny models to 1.5B params for large.",
        "reference_link": "https://github.com/openai/whisper"
      }
    },
    "bosonai-higgs-audio-v2-generation-3B-base": {
      "model_id": "bosonai-higgs-audio-v2-generation-3B-base",
      "model_type": "audio",
      "description": "Higgs Audio v2 Generation 3B Base is a powerful multimodal audio generation model that supports voice cloning, smart voice generation, and multi-speaker synthesis. Built on vLLM engine with OpenAI-compatible API for text-to-speech and audio generation tasks.",
      "application_scenario": "voice cloning, text-to-speech, audio generation, multi-speaker synthesis, smart voice generation",
      "supported_instances": [
        "g5.48xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "bosonai/higgs-audio-v2-generation-3B-base",
      "modelscope_model_id": "",
      "model_series": null
    },
    "bce-embedding-base_v1": {
      "model_id": "bce-embedding-base_v1",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g4dn.2xlarge",
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "InfiniFlow/bce-embedding-base_v1",
      "modelscope_model_id": "maidalun/bce-embedding-base_v1",
      "model_series": {
        "name": "bce",
        "description": "BCEmbedding: Bilingual and Crosslingual Embedding for RAG.",
        "reference_link": "https://huggingface.co/maidalun1020/bce-embedding-base_v1"
      }
    },
    "bge-base-en-v1.5": {
      "model_id": "bge-base-en-v1.5",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/bge-base-en-v1.5",
      "modelscope_model_id": "BAAI/bge-base-en-v1.5",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "bge-m3": {
      "model_id": "bge-m3",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "local",
        "ecs"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/bge-m3",
      "modelscope_model_id": "BAAI/bge-m3",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "jina-embeddings-v3": {
      "model_id": "jina-embeddings-v3",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "huggingface"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "jinaai/jina-embeddings-v3",
      "modelscope_model_id": "jinaai/jina-embeddings-v3",
      "model_series": {
        "name": "jina",
        "description": "Search foundation models: embeddings, rerankers, small LMs for better search",
        "reference_link": "https://huggingface.co/jinaai"
      }
    },
    "jina-embeddings-v4-vllm-retrieval": {
      "model_id": "jina-embeddings-v4-vllm-retrieval",
      "model_type": "embedding",
      "description": "jina-embeddings-v4 is a universal embedding model for multimodal and multilingual retrieval",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "jinaai/jina-embeddings-v4-vllm-retrieval",
      "modelscope_model_id": "jinaai/jina-embeddings-v4-vllm-retrieval",
      "model_series": {
        "name": "jina",
        "description": "Search foundation models: embeddings, rerankers, small LMs for better search",
        "reference_link": "https://huggingface.co/jinaai"
      }
    },
    "Qwen3-Embedding-0.6B": {
      "model_id": "Qwen3-Embedding-0.6B",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-Embedding-0.6B",
      "modelscope_model_id": "Qwen/Qwen3-Embedding-0.6B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-Embedding-4B": {
      "model_id": "Qwen3-Embedding-4B",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-Embedding-4B",
      "modelscope_model_id": "Qwen/Qwen3-Embedding-4B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "Qwen3-Embedding-8B": {
      "model_id": "Qwen3-Embedding-8B",
      "model_type": "embedding",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Qwen/Qwen3-Embedding-8B",
      "modelscope_model_id": "Qwen/Qwen3-Embedding-8B",
      "model_series": {
        "name": "qwen3",
        "description": "the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.",
        "reference_link": "https://github.com/QwenLM/Qwen3"
      }
    },
    "gme-Qwen2-VL-7B-Instruct": {
      "model_id": "gme-Qwen2-VL-7B-Instruct",
      "model_type": "embedding",
      "description": "General Multimodal Embedding model based on Qwen2-VL architecture, supporting text, image, and image-text pair inputs for unified multimodal representation learning and retrieval tasks. Uses vLLM v0.8.4 for transformers compatibility.",
      "application_scenario": "Multimodal RAG, image-text retrieval, visual search",
      "supported_instances": [
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "Alibaba-NLP/gme-Qwen2-VL-7B-Instruct",
      "modelscope_model_id": "Alibaba-NLP/gme-Qwen2-VL-7B-Instruct",
      "model_series": {
        "name": "gme",
        "description": "General Multimodal Embedding (GME) models based on Qwen2-VL architecture, designed for unified multimodal representation learning supporting text, image, and image-text pair inputs for retrieval and search applications.",
        "reference_link": "https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct"
      }
    },
    "bge-vl-base": {
      "model_id": "bge-vl-base",
      "model_type": "embedding",
      "description": "BGE-VL-base is a multimodal embedding model that supports text, image, and text-image pair inputs for unified multimodal representation learning and cross-modal retrieval tasks. Lightweight with 149M parameters.",
      "application_scenario": "Multimodal RAG, composed image retrieval, visual search",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "huggingface"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/BGE-VL-base",
      "modelscope_model_id": "BAAI/BGE-VL-base",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "bge-vl-large": {
      "model_id": "bge-vl-large",
      "model_type": "embedding",
      "description": "BGE-VL-large is a larger multimodal embedding model that supports text, image, and text-image pair inputs for high-performance multimodal representation learning and cross-modal retrieval tasks.",
      "application_scenario": "Multimodal RAG, composed image retrieval, visual search",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "huggingface"
      ],
      "supported_services": [
        "ecs"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/BGE-VL-large",
      "modelscope_model_id": "BAAI/BGE-VL-large",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "bge-reranker-v2-m3": {
      "model_id": "bge-reranker-v2-m3",
      "model_type": "rerank",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g4dn.2xlarge",
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/bge-reranker-v2-m3",
      "modelscope_model_id": "BAAI/bge-reranker-v2-m3",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "bge-reranker-large": {
      "model_id": "bge-reranker-large",
      "model_type": "rerank",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g4dn.2xlarge",
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "vllm"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "BAAI/bge-reranker-large",
      "modelscope_model_id": "BAAI/bge-reranker-large",
      "model_series": {
        "name": "bge",
        "description": "BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, it currently includes kinds of embedding/rerank models.",
        "reference_link": "https://github.com/FlagOpen/FlagEmbedding"
      }
    },
    "jina-reranker-v2-base-multilingual": {
      "model_id": "jina-reranker-v2-base-multilingual",
      "model_type": "rerank",
      "description": "",
      "application_scenario": "RAG",
      "supported_instances": [
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.16xlarge",
        "local"
      ],
      "supported_engines": [
        "huggingface"
      ],
      "supported_services": [
        "sagemaker_realtime",
        "ecs",
        "local"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "jinaai/jina-reranker-v2-base-multilingual",
      "modelscope_model_id": "jinaai/jina-reranker-v2-base-multilingual",
      "model_series": {
        "name": "jina",
        "description": "Search foundation models: embeddings, rerankers, small LMs for better search",
        "reference_link": "https://huggingface.co/jinaai"
      }
    },
    "custom-docker": {
      "model_id": "custom-docker",
      "model_type": "llm",
      "description": "Custom model running in Docker container",
      "application_scenario": "",
      "supported_instances": [
        "g4dn.xlarge",
        "g4dn.2xlarge",
        "g4dn.4xlarge",
        "g4dn.8xlarge",
        "g4dn.12xlarge",
        "g4dn.16xlarge",
        "g5.xlarge",
        "g5.2xlarge",
        "g5.4xlarge",
        "g5.8xlarge",
        "g5.12xlarge",
        "local"
      ],
      "supported_engines": [
        "custom"
      ],
      "supported_services": [
        "sagemaker_realtime"
      ],
      "allow_china_region": true,
      "huggingface_model_id": "",
      "modelscope_model_id": "",
      "model_series": null
    }
  },
  "instances": {
    "g4dn.xlarge": {
      "instance_type": "g4dn.xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 4,
      "memory": 16,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g4dn.2xlarge": {
      "instance_type": "g4dn.2xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 8,
      "memory": 32,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g4dn.4xlarge": {
      "instance_type": "g4dn.4xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 16,
      "memory": 64,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g4dn.8xlarge": {
      "instance_type": "g4dn.8xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 32,
      "memory": 128,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g4dn.16xlarge": {
      "instance_type": "g4dn.16xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 64,
      "memory": 256,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g4dn.12xlarge": {
      "instance_type": "g4dn.12xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 48,
      "memory": 192,
      "description": "Amazon EC2 G4 instances are the industry’s most cost-effective and versatile GPU instances for deploying machine learning models such as image classification, object detection, and speech recognition, and for graphics-intensive applications such as remote graphics workstations, game streaming, and graphics rendering.",
      "support_cn_region": true
    },
    "g5.xlarge": {
      "instance_type": "g5.xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 4,
      "memory": 16,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.2xlarge": {
      "instance_type": "g5.2xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 8,
      "memory": 32,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.4xlarge": {
      "instance_type": "g5.4xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 16,
      "memory": 64,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.8xlarge": {
      "instance_type": "g5.8xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 32,
      "memory": 128,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.12xlarge": {
      "instance_type": "g5.12xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 48,
      "memory": 192,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.16xlarge": {
      "instance_type": "g5.16xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 64,
      "memory": 256,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.24xlarge": {
      "instance_type": "g5.24xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 96,
      "memory": 384,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g5.48xlarge": {
      "instance_type": "g5.48xlarge",
      "gpu_num": 8,
      "neuron_core_num": null,
      "vcpu": 192,
      "memory": 768,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": true
    },
    "g6.xlarge": {
      "instance_type": "g6.xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 4,
      "memory": 16,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.2xlarge": {
      "instance_type": "g6.2xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 8,
      "memory": 32,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.4xlarge": {
      "instance_type": "g6.4xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 16,
      "memory": 64,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.8xlarge": {
      "instance_type": "g6.8xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 32,
      "memory": 128,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.12xlarge": {
      "instance_type": "g6.12xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 48,
      "memory": 192,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.16xlarge": {
      "instance_type": "g6.16xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 64,
      "memory": 256,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.24xlarge": {
      "instance_type": "g6.24xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 96,
      "memory": 384,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6.48xlarge": {
      "instance_type": "g6.48xlarge",
      "gpu_num": 8,
      "neuron_core_num": null,
      "vcpu": 192,
      "memory": 768,
      "description": "Amazon EC2 G5 instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton5 processors.",
      "support_cn_region": false
    },
    "g6e.xlarge": {
      "instance_type": "g6e.xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 4,
      "memory": 32,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.2xlarge": {
      "instance_type": "g6e.2xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 8,
      "memory": 64,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.4xlarge": {
      "instance_type": "g6e.4xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 16,
      "memory": 128,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.8xlarge": {
      "instance_type": "g6e.8xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 32,
      "memory": 256,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.16xlarge": {
      "instance_type": "g6e.16xlarge",
      "gpu_num": 1,
      "neuron_core_num": null,
      "vcpu": 64,
      "memory": 512,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.12xlarge": {
      "instance_type": "g6e.12xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 48,
      "memory": 384,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.24xlarge": {
      "instance_type": "g6e.24xlarge",
      "gpu_num": 4,
      "neuron_core_num": null,
      "vcpu": 96,
      "memory": 768,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "g6e.48xlarge": {
      "instance_type": "g6e.48xlarge",
      "gpu_num": 8,
      "neuron_core_num": null,
      "vcpu": 192,
      "memory": 1536,
      "description": "Amazon EC2 G6e instances are powered by the latest generation of Amazon GPU-optimized processors, the AWS Graviton6 processors.",
      "support_cn_region": false
    },
    "inf2.xlarge": {
      "instance_type": "inf2.xlarge",
      "gpu_num": null,
      "neuron_core_num": 2,
      "vcpu": 4,
      "memory": 16,
      "description": "Amazon Inferentia2 chips.",
      "support_cn_region": false
    },
    "inf2.8xlarge": {
      "instance_type": "inf2.8xlarge",
      "gpu_num": null,
      "neuron_core_num": 2,
      "vcpu": 32,
      "memory": 128,
      "description": "Amazon Inferentia2 chips.",
      "support_cn_region": false
    },
    "inf2.24xlarge": {
      "instance_type": "inf2.24xlarge",
      "gpu_num": null,
      "neuron_core_num": 12,
      "vcpu": 96,
      "memory": 384,
      "description": "Amazon Inferentia2 chips.",
      "support_cn_region": false
    },
    "inf2.48xlarge": {
      "instance_type": "inf2.48xlarge",
      "gpu_num": null,
      "neuron_core_num": 24,
      "vcpu": 192,
      "memory": 768,
      "description": "Amazon Inferentia2 chips.",
      "support_cn_region": false
    },
    "local": {
      "instance_type": "local",
      "gpu_num": null,
      "neuron_core_num": null,
      "vcpu": 0,
      "memory": 0,
      "description": "local instance",
      "support_cn_region": true
    }
  },
  "engines": {
    "vllm": {
      "engine_type": "vllm",
      "description": "vLLM - High-performance inference engine optimized for large language models",
      "support_inf2_instance": false
    },
    "huggingface": {
      "engine_type": "huggingface",
      "description": "Hugging Face Transformers - Popular library for transformer models",
      "support_inf2_instance": false
    },
    "tgi": {
      "engine_type": "tgi",
      "description": "Text Generation Inference - Optimized inference server for text generation",
      "support_inf2_instance": true
    },
    "ollama": {
      "engine_type": "ollama",
      "description": "Ollama - Lightweight, high-performance model server",
      "support_inf2_instance": false
    },
    "llama.cpp": {
      "engine_type": "llama.cpp",
      "description": "llama.cpp - Efficient C++ implementation for LLaMA models",
      "support_inf2_instance": false
    },
    "lmdeploy": {
      "engine_type": "lmdeploy",
      "description": "LMDeploy - High-performance inference engine",
      "support_inf2_instance": false
    },
    "comfyui": {
      "engine_type": "comfyui",
      "description": "ComfyUI - Node-based interface for stable diffusion workflows",
      "support_inf2_instance": false
    },
    "ktransformers": {
      "engine_type": "ktransformers",
      "description": "KTransformers - Optimized transformer inference engine",
      "support_inf2_instance": false
    }
  },
  "services": {
    "sagemaker_realtime": {
      "service_type": "sagemaker_realtime",
      "name": "Amazon SageMaker AI Real-time inference with OpenAI-Compatible API",
      "description": "Amazon SageMaker Real-time inference provides low-latency, interactive inference through fully managed endpoints that support autoscaling. It provides an OpenAI-compatible REST API (e.g., /v1/completions) via an Application Load Balancer (ALB).\n(https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)",
      "support_cn_region": true,
      "need_vpc": false
    },
    "sagemaker": {
      "service_type": "sagemaker",
      "name": "Amazon SageMaker AI Real-time inference",
      "description": "Amazon SageMaker Real-time inference provides low-latency, interactive inference through fully managed endpoints that support autoscaling. \n(https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)",
      "support_cn_region": true,
      "need_vpc": false
    },
    "sagemaker_async": {
      "service_type": "sagemaker_async",
      "name": "Amazon SageMaker AI Asynchronous inference with OpenAI-Compatible API",
      "description": "Amazon SageMaker Asynchronous Inference queues requests for processing asynchronously, making it suitable for large payloads (up to 1GB) and long processing times (up to one hour), while also enabling cost savings by autoscaling to zero when idle. It provides an OpenAI-compatible REST API (e.g., /v1/completions) via an Application Load Balancer (ALB).\n(https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)",
      "support_cn_region": true,
      "need_vpc": false
    },
    "ec2": {
      "service_type": "ec2",
      "name": "Amazon EC2",
      "description": "Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud.",
      "support_cn_region": false,
      "need_vpc": true
    },
    "ecs": {
      "service_type": "ecs",
      "name": "Amazon ECS with OpenAI-Compatible API",
      "description": "Amazon Elastic Container Service is a fully managed service that runs containerized applications in clusters with auto scaling. It provides an OpenAI-compatible REST API (e.g., /v1/completions) via an Application Load Balancer (ALB), enabling integration with AI models for tasks like chatbots or document analysis. (https://docs.aws.amazon.com/AmazonECS/latest/developerguide)",
      "support_cn_region": true,
      "need_vpc": true
    },
    "local": {
      "service_type": "local",
      "name": "Local",
      "description": "",
      "support_cn_region": true,
      "need_vpc": false
    }
  }
};

// Helper functions for accessing configuration
window.EMD_HELPERS = {
  getModel: function(modelId) {
    return window.EMD_MODEL_CONFIG.models[modelId];
  },

  getAllModels: function() {
    return window.EMD_MODEL_CONFIG.models;
  },

  getModelsByType: function(type) {
    const models = {};
    for (const [id, model] of Object.entries(window.EMD_MODEL_CONFIG.models)) {
      if (model.model_type === type) {
        models[id] = model;
      }
    }
    return models;
  },

  getInstance: function(instanceType) {
    return window.EMD_MODEL_CONFIG.instances[instanceType];
  },

  getEngine: function(engineType) {
    return window.EMD_MODEL_CONFIG.engines[engineType];
  },

  getService: function(serviceType) {
    return window.EMD_MODEL_CONFIG.services[serviceType];
  },

  getCompatibleInstances: function(modelId) {
    const model = this.getModel(modelId);
    return model ? model.supported_instances : [];
  },

  getCompatibleEngines: function(modelId) {
    const model = this.getModel(modelId);
    return model ? model.supported_engines : [];
  },

  getCompatibleServices: function(modelId) {
    const model = this.getModel(modelId);
    return model ? model.supported_services : [];
  },

  getModelCount: function() {
    return Object.keys(window.EMD_MODEL_CONFIG.models).length;
  },

  getGeneratedAt: function() {
    return window.EMD_MODEL_CONFIG.metadata.generated_at;
  },

  // Additional utility functions
  getModelsByEngine: function(engineType) {
    const models = {};
    for (const [id, model] of Object.entries(window.EMD_MODEL_CONFIG.models)) {
      if (model.supported_engines.includes(engineType)) {
        models[id] = model;
      }
    }
    return models;
  },

  getModelsByInstance: function(instanceType) {
    const models = {};
    for (const [id, model] of Object.entries(window.EMD_MODEL_CONFIG.models)) {
      if (model.supported_instances.includes(instanceType)) {
        models[id] = model;
      }
    }
    return models;
  },

  getModelsByService: function(serviceType) {
    const models = {};
    for (const [id, model] of Object.entries(window.EMD_MODEL_CONFIG.models)) {
      if (model.supported_services.includes(serviceType)) {
        models[id] = model;
      }
    }
    return models;
  },

  getChinaRegionModels: function() {
    const models = {};
    for (const [id, model] of Object.entries(window.EMD_MODEL_CONFIG.models)) {
      if (model.allow_china_region) {
        models[id] = model;
      }
    }
    return models;
  },

  getInstanceSpecs: function(instanceType) {
    const instance = this.getInstance(instanceType);
    if (!instance) return null;

    const specs = [];
    if (instance.gpu_num) specs.push(`${instance.gpu_num} GPU`);
    if (instance.neuron_core_num) specs.push(`${instance.neuron_core_num} Neuron`);
    specs.push(`${instance.vcpu} vCPU`);
    specs.push(`${instance.memory} GiB`);

    return {
      text: specs.join(', '),
      gpu_num: instance.gpu_num,
      neuron_core_num: instance.neuron_core_num,
      vcpu: instance.vcpu,
      memory: instance.memory
    };
  }
};

console.log(`✅ EMD Model Configuration loaded: ${window.EMD_HELPERS.getModelCount()} models available`);
console.log(`📊 Model types: LLM (${Object.keys(window.EMD_HELPERS.getModelsByType('llm')).length}), VLM (${Object.keys(window.EMD_HELPERS.getModelsByType('vlm')).length}), Embedding (${Object.keys(window.EMD_HELPERS.getModelsByType('embedding')).length}), Whisper (${Object.keys(window.EMD_HELPERS.getModelsByType('whisper')).length})`);
